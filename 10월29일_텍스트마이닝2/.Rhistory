sidebarPanel(
radioButtons("x", "Select X-axis:",
list("Sepal.Length"='a',
"Sepal.Width"='b',
"Petal.Length"='c',
"Petal.Width"='d')),
radioButtons("y", "Select Y-axis:",
list("Spal.Length"='e',
"Sepal.Width"='f',
"Petal.Length"='g',
"Petal.Width"='h'))
),
mainPanel(
plotOutput("myPlot")
)
)
)
)
#SERVER.R
library(shiny)
shinyServer(function(input, output) {
output$distPlot <- renderPlot({
if(input$x=='a'){
i<-1
}
if(input$x=='b'){
i<-2
}
if(input$x=='c'){
i<-3
}
if(input$x=='d'){
i<-4
}
if(input$y=='e'){
j<-1
}
if(input$y=='f'){
j<-2
}
if(input$y=='g'){
j<-3
}
if(input$y=='h'){
j<-4
}
s    <- iris[, i]
k    <- iris[, j]
plot(s,k)
})
})
shiny::runApp("drawplot")
shiny::runApp("drawplot")
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
shiny::runapp("drawplot")
shiny::runApp("drawplot")
shinyApp(ui=ui,server=server)
shiny::runApp("drawplot")
install.packages("devtools")
devtools::install_packages("rstudio/shiny")
library(runGithub)
install.packages("runGithub")
install.packages("shinydashboard")
library(shinydashboard)
runExample("01_hello")
runExample("04_mpg")
runExample("08_html")
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
install.packages(c('shiny', 'ggvis', 'dplyr', 'RSQLite'))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
runGitHub(repo="shiny-examples", username="rstudio",
subdir="051-movie-explorer")
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
library(C(shiny, ggvis, dplyr, RSQLLite))
library(shinydashboard)
library(runGithub)
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
install.packages("runGithub")
install.packages("shinydashboard")
install.packages("shinydashboard")
library(runGithub)
library(shinydashboard)
runExample()
runExample("01_hello")
runExample("04_mpg")
runExample("08_html")
library(tm)
getwd()
setwd("C:\Users/ktm/Documents/choi_dontouch/R-basic/10월29일_텍스트마이닝2/dir_multi")
setwd("C:/Users/ktm/Documents/choi_dontouch/R-basic/10월29일_텍스트마이닝2/dir_multi")
data <- readLines("ratings01.txt")
data
####텍스트에서 말뭉치로 변환
tSource <- vectorSource(date)
####텍스트에서 말뭉치로 변환
tSource <- VectorSource(date)
mycor <- Corpus(tsource)
mycor <- Corpus(tSource)
####텍스트에서 말뭉치로 변환
tSource <- VectorSource(date)
mycor <- Corpus(tSource)
####텍스트에서 말뭉치로 변환
tSource <- VectorSource(data)
mycor <- Corpus(tSource)
inspect(mycor)
mycor <- tm_map(mycor, removePunctuation)
mycor <- tm_map(mycor, removeNumbers)
myCor <- tm_map(myCor, stripWhitespace)
myCor <- tm_map(mycor, stripWhitespace)
inspect(myCor)
##불용어 처리
myCorpus <- tm_map(myCorpus, removeWords, c("영화","ㅎㅎㅎ","여러"))
##불용어 처리
myCorpus <- tm_map(myCor, removeWords, c("영화","ㅎㅎㅎ","여러"))
myCorpus
myCorpus[[1]]$content
inspect(myCorpus)
##불용어 처리
myCorpus <- tm_map(myCor, removeWords, c("영화","ㅋㅋ","여러"))
inspect(myCorpus)
###
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
##불용어 처리
myCorpus <- tm_map(myCor, removeWords, c("영화","ㅋㅋ","여러"))
myCorpus[[1]]$contentn <- gsub("ㅋㅋㅋ","", myCorpus[[1]]$content)
myCorpus[[i]]$content <- gsub("영화","", myCorpus[[i]]$content)
for ( i in seq_along(myCorpus)){
myCorpus[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("영화","", myCorpus[[i]]$content)
}
###
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
wordFreq <- sort(myCorpus, decreasing=T)
wordFreq <- sort(myCorpus)
wordFreq <- sort(myCorpus[[1]]$content)
wordFreq <- sort(myCorpus[[1]]$content, decreasing=T)
wordFreq
mycor <- Corpus(tSource)
mycor <- tm_map(mycor, removePunctuation)
mycor <- tm_map(mycor, removeNumbers)
myCor <- tm_map(mycor, stripWhitespace)
inspect(myCor)
for ( i in seq_along(myCorpus)){
myCorpus[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("영화","", myCorpus[[i]]$content)
}
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
library(tm)
library(KoNLP)
setwd("C:/Users/ktm/Documents/choi_dontouch/R-basic/10월29일_텍스트마이닝2")
myCorpus <- VCorpus(DirSource("dir_multi", pattern="txt"))
inspect(myCorpus)
str(myCorpus)
content(myCorpus[[1]])
###문서의 내용 전체를 보겠다
lapply(myCorpus, content)
###각각의 문서의 리뷰를 한줄로 만들겠다
for(i in seq_along(myCorpus)){
myCorpus[[i]]$content <- paste(myCorpus[[i]]$content, collapse=" ")
}
myCorpus <- tm_map(myCorpus, stripWhitespace) #공백제거
#myCorpus <- tm_map(myCorpus, tolower)         ##소문자로
myCorpus <- tm_map(myCorpus, removePunctuation)##특수문자제거
myCorpus <- tm_map(myCorpus, removeNumbers) ##숫자제거
library(NIADic)
for ( i in seq_along(myCorpus)){
myCorpus[[i]]$contentn <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCorpus[[i]]$content)
}
for ( i in seq_along(myCorpus)){
myCorpus[[i]]$contentn <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCorpus[[i]]$content)
myCorpus[[i]]$contentn <- gsub("ㅋㅋㅋ","", myCorpus[[i]]$content)
myCorpus[[i]]$contentn <- gsub("영화","", myCorpus[[i]]$content)
}
extractNoun(myCorpus[[1]]$content)
nouns <- extractNoun(myCorpus[[1]]$content)
nouns <- nouns[nchar(nouns) > 2]
nouns <- paste(nouns, collapse=" ")
nouns
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
nTerms(myCor_tdm)
nDocs(myCor_tdm)
Terms(myCor_tdm)[nchar(Terms(myCor_tdm)) >= 2 & nchar(Terms(myCor_tdm)) <= 4]
findFreqTerms(myCor_tdm, lowfreq=3, highfreq=Inf)
findAssocs(myCor_tdm, c("택시", "중국영화"), c(0.5))
myCor_tdm <- removeSparseTerms(myCor_tdm, sparse=0.90)
myCor_tdm
wordFreq = slam::row_sums(myCor_tdm)
wordFreq = sort(wordFreq, decreasing=TRUE)
wordFreq
library(wordcloud)
pal <- brewer.pal(8, "Dark2")
w = names(wordFreq)
wordcloud(words=w, freq=wordFreq,
min.freq=3, random.order=F,
random.color=T, colors=pal)
rm.idx <- grep("[있습니다|이거|영화|나는|보는|그런|그냥|것을]",names(wordFreq))
wordFreq1 <- wordFreq[-rm.idx]
wordFreq1
w1 <- names(wordFreq1)
wordcloud(words=w1, freq=wordFreq,
min.freq=3, random.order=F,
random.color=T, colors=pal)
tds <- myCor_tdm[Terms(myCor_tdm) %in% w1, ]
m2 <- as.matrix(tds)
m2 <- as.matrix(tds)
colnames(m2) <- gsub(".txt", "", colnames(m2))
distMatrix <- dist(scale(m2))
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
mycor <- Corpus(tSource)
mycor <- tm_map(mycor, removePunctuation)
mycor <- tm_map(mycor, removeNumbers)
myCor <- tm_map(mycor, stripWhitespace)
##불용어 처리
myCorpus <- tm_map(myCor, removeWords, c("영화","ㅋㅋ","여러"))
for ( i in seq_along(myCorpus)){
myCorpus[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("영화","", myCorpus[[i]]$content)
}
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
tdm
m <- as.matrix(tdm)
m
nTerms(tdm)
inspect(tdm)
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
p
myCorpus
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- paste(myCorpus[[i]]$content, collapse=" ")
}
myCorpus[[5]]$content
myCorpus <- tm_map(myCorpus, removePunctuation)     # 특수문자 제거
myCorpus <- tm_map(myCorpus, removeNumbers)         # 숫자 제거하기
myCorpus <- tm_map(myCorpus, stripWhitespace)       # 공백제거
myCorpus[[1]]$content
myCorpus[[5]]$content
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("볼 수록", "볼수록", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("이쁨터짐", "이쁨 터짐", myCorpus[[i]]$content )
}
myCorpus[[3]]$content
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
myCorpus <- VCorpus(DirSource("dir_multi", pattern="txt"))
myCorpus
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- paste(myCorpus[[i]]$content, collapse=" ")
}
myCorpus[[5]]$content
myCorpus <- tm_map(myCorpus, removePunctuation)     # 특수문자 제거
myCorpus <- tm_map(myCorpus, removeNumbers)         # 숫자 제거하기
myCorpus <- tm_map(myCorpus, stripWhitespace)       # 공백제거
myCorpus[[1]]$content
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english')) ## 불용어 제거하기
myCorpus[[5]]$content
library(SnowballC)
myCorpus[[5]]$content
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("볼 수록", "볼수록", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("이쁨터짐", "이쁨 터짐", myCorpus[[i]]$content )
}
myCorpus[[3]]$content
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
nTerms(myCor_tdm)
nDocs(myCor_tdm)
Terms(myCor_tdm)[nchar(Terms(myCor_tdm)) >= 2 & nchar(Terms(myCor_tdm)) <= 4]
findFreqTerms(myCor_tdm, lowfreq=3, highfreq=Inf)
findAssocs(myCor_tdm, c("택시", "중국영화"), c(0.5))
m
####텍스트에서 말뭉치로 변환
tSource <- VectorSource(data)
mycor <- Corpus(tSource)
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
mycor <- Corpus(tSource)
inspect(mycor)
myCor <- Corpus(tSource)
inspect(myCor)
for ( i in seq_along(myCor)){
myCor[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCor[[i]]$content)
myCor[[i]]$content <- gsub("영화","", myCor[[i]]$content)
}
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
m <- as.matrix(tdm)
m
frequency <- rowSums(myCor_tdm)
wordFreq = slam::row_sums(myCor_tdm)
#extractNoun(myCorpus[[1]]$content)
#nouns[nchar(nouns) > 2]
#paste(nouns, collapse=" ")
myCor_tdm
inspect(myCor_tdm)
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
####텍스트에서 말뭉치로 변환
tSource <- VectorSource(data)
myCor <- Corpus(tSource)
inspect(mycor)
for ( i in seq_along(myCor)){
myCor[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCor[[i]]$content)
myCor[[i]]$content <- gsub("영화","", myCor[[i]]$content)
}
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
mycor <- tm_map(mycor, removePunctuation)
mycor <- tm_map(mycor, removeNumbers)
for ( i in seq_along(myCor)){
myCor[[i]]$content <- gsub("ㅎㅎㅎㅎㅎㅎㅎㅎ","", myCor[[i]]$content)
myCor[[i]]$content <- gsub("영화","", myCor[[i]]$content)
}
###매트릭스 만들기
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
library(rvest)
rm(list=ls())
all.reviews <-c()
all_url <-c()
for (i in 1:10 ){
miss <- paste0('https://movie.daum.net/moviedb/grade?movieId=106812&type=netizen&page=',i)
all_url <- c(all_url,miss)
htxt1=read_html(all_url[i])
table1=html_nodes(htxt1,'.review_info')
content1=html_nodes(table1,'.desc_review')
reviews1 = html_text(content1)
reviews1 <-gsub('\n','',reviews1)
reviews1 <-gsub('\t','',reviews1)
all.reviews  <-c(all.reviews , reviews1)
}
all.reviews
####명사추출
data <- all_reviews
####명사추출
data <- all.reviews
tSource <- VectorSource(data)
myCor <- Corpus(tSource)
inspect(mycor)
inspect(myCor)
myCorpus <- Corpus(tSource)
inspect(myCor)
myCorpus <- Corpus(tSource)
inspect(myCorpus)
myCorpus[[1]]$content
myCorpus[[2]]$content
myCorpus[[3]]$content
myCorpus[[120]]$content
myCorpus[[12]]$content
inspect(myCorpus)
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("영화", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("\r", " ", myCorpus[[i]]$content )
}
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
myCorpus <- Corpus(tSource)
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
Encoding(tdm$dimnames$Terms)='UTF-8'
inspect(tdm)
m <- as.matrix(tdm)
m
Encoding(myCorpus$dimnames$Terms)='UTF-8'
frequency <- rowSums(myCor_tdm)
myCor_tdm <- removeSparseTerms(myCor_tdm, sparse=0.90)
myCorpus <- VCorpus(DirSource("dir_multi", pattern="txt"))
myCorpus
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- paste(myCorpus[[i]]$content, collapse=" ")
}
myCorpus[[5]]$content
myCorpus[[5]]$content
myCorpus <- tm_map(myCorpus, removePunctuation)     # 특수문자 제거
myCorpus <- tm_map(myCorpus, removeNumbers)         # 숫자 제거하기
myCorpus[[1]]$content
#extractNoun(myCorpus[[1]]$content)
#nouns[nchar(nouns) > 2]
#paste(nouns, collapse=" ")
myCor_tdm
myCor_tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(myCor_tdm)
nTerms(myCor_tdm)
nDocs(myCor_tdm)
Terms(myCor_tdm)[nchar(Terms(myCor_tdm)) >= 2 & nchar(Terms(myCor_tdm)) <= 4]
findFreqTerms(myCor_tdm, lowfreq=3, highfreq=Inf)
findAssocs(myCor_tdm, c("택시", "중국영화"), c(0.5))
frequency <- rowSums(myCor_tdm)
frequency <- row_Sums(myCor_tdm)
wordFreq = slam::row_sums(myCor_tdm)
?slam
rm(list=ls())
all.reviews <-c()
all_url <-c()
for (i in 1:100 ){
miss <- paste0('https://movie.daum.net/moviedb/grade?movieId=110097&type=netizen&page=',i)
all_url <- c(all_url,miss)
htxt1=read_html(all_url[i])
table1=html_nodes(htxt1,'.review_info')
content1=html_nodes(table1,'.desc_review')
reviews1 = html_text(content1)
reviews1 <-gsub('\n','',reviews1)
reviews1 <-gsub('\t','',reviews1)
all.reviews  <-c(all.reviews , reviews1)
}
all.reviews
rm(list=ls())
all.reviews <-c()
all_url <-c()
for (i in 1:20 ){
miss <- paste0('https://movie.daum.net/moviedb/grade?movieId=110097&type=netizen&page=',i)
all_url <- c(all_url,miss)
htxt1=read_html(all_url[i])
table1=html_nodes(htxt1,'.review_info')
content1=html_nodes(table1,'.desc_review')
reviews1 = html_text(content1)
reviews1 <-gsub('\n','',reviews1)
reviews1 <-gsub('\t','',reviews1)
all.reviews  <-c(all.reviews , reviews1)
}
all.reviews
install.packages("devtools")
install.packages("devtools")
####명사추출
data <- all.reviews
tSource <- VectorSource(data)
myCorpus <- Corpus(tSource)
inspect(myCorpus)
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("영화", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("\r", " ", myCorpus[[i]]$content )
}
inspect(myCorpus)
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("영화", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("\r", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("ㅋㅋㅋㅋㅋ", " ", myCorpus[[i]]$content )
}
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
Encoding(tdm$dimnames$Terms)='UTF-8'
Encoding(tdm$dimnames$Terms)='UTF-8'
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
inspect(myCorpus)
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("영화", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("\r", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("ㅋㅋㅋㅋㅋ", " ", myCorpus[[i]]$content )
}
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
Encoding(tdm$dimnames$Terms)='UTF-8'
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
Encoding(myCorpus$dimnames$Terms)='UTF-8'
myCorpus <- Corpus(tSource)
inspect(myCorpus)
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("영화", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("\r", " ", myCorpus[[i]]$content )
myCorpus[[i]]$content <- gsub("ㅋㅋㅋㅋㅋ", " ", myCorpus[[i]]$content )
}
Encoding(tdm$dimnames$Terms)='UTF-8'
tdm <- TermDocumentMatrix(myCorpus, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("tm")
library(tm)
####명사추출
data <- all.reviews
tSource <- VectorSource(data)
myCorpus <- Corpus(tSource)
library(tm)
install.packages("tm")
install.packages("tm")
library(tm)
