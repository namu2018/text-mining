"Sepal.Width"='f',
"Petal.Length"='g',
"Petal.Width"='h'))
),
mainPanel(
plotOutput("myPlot")
)
)
)
)
#SERVER.R
library(shiny)
shinyServer(function(input, output) {
output$distPlot <- renderPlot({
if(input$x=='a'){
i<-1
}
if(input$x=='b'){
i<-2
}
if(input$x=='c'){
i<-3
}
if(input$x=='d'){
i<-4
}
if(input$y=='e'){
j<-1
}
if(input$y=='f'){
j<-2
}
if(input$y=='g'){
j<-3
}
if(input$y=='h'){
j<-4
}
s    <- iris[, i]
k    <- iris[, j]
plot(s,k)
})
})
shiny::runApp("drawplot")
shiny::runApp("drawplot")
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
runApp('drawplot')
shiny::runapp("drawplot")
shiny::runApp("drawplot")
shinyApp(ui=ui,server=server)
shiny::runApp("drawplot")
install.packages("devtools")
devtools::install_packages("rstudio/shiny")
library(runGithub)
install.packages("runGithub")
install.packages("shinydashboard")
library(shinydashboard)
runExample("01_hello")
runExample("04_mpg")
runExample("08_html")
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
install.packages(c('shiny', 'ggvis', 'dplyr', 'RSQLite'))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
runGitHub(repo="shiny-examples", username="rstudio",
subdir="051-movie-explorer")
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
library(C(shiny, ggvis, dplyr, RSQLLite))
library(shinydashboard)
library(runGithub)
runGitHub(repo="shiny-examples", username="rstudio",
subdir="086-bus-dashboard")
install.packages("runGithub")
install.packages("shinydashboard")
install.packages("shinydashboard")
library(runGithub)
library(shinydashboard)
runExample()
runExample("01_hello")
runExample("04_mpg")
runExample("08_html")
install.packages("KoNLP")
library(KoNLP)
useSejongDic()
sentence <- "우리나라 고유 문화는 우수하다. 전시회를 재미있게 보았다. 시간이 스스륵 지나갔다."
extractNoun(sentence)
extractNoun(sentence, autoSpacing = TRUE)
##사전에 단어 추가
##mergeUserDic()함수를 사용하여 sejong사전에 '스르륵' 단어 '부사'로 추가하기
mergeUserDic(data.frame(c('스르륵'),c('mag')))
extractNoun(sentence, autoSpacing = TRUE)
extractNoun(sentence, autoSpacing = FALSE)##기본
##명사추출
extractNoun(sentence)
##사전에 단어 추가
##mergeUserDic()함수를 사용하여 sejong사전에 '스르륵' 단어 '부사'로 추가하기
mergeUserDic(data.frame(c('스르륵'),c('mag')))
##명사추출
extractNoun(sentence)
###형태소 분석하기
MorphAnalyzer(sentence)
###형태소 분석하기
MorphAnalyzer(sentence)
###형태소 분석하기
Mor <- MorphAnalyzer(sentence)
ls(Mor)
names(Mor)
###품사태그를 달아주는 함수
SimplePos09(sentence)
SimplePos22(sentence)
Mor
class(Mor)
###품사태그를 달아주는 함수
SimplePos09(sentence)
getwd()
setwd("C:/Users/ktm/Documents/choi_dontouch/R-basic/10월25일_텍스트 마이닝")
sen <- read.txt("미국과중국.txt")
sen <- readLines("미국과중국.txt")
sen
extractNoun(sen)
sen <- extractNoun(sen)
sen
###명사확인
library(stringer)
###명사확인
library(stringr)
txt=SimplePos09(sen)
txt
txt_n <- str_match(txt, '([a-Z가-힣]+)/n')
txt_n <- str_match(txt, '([a-ZA-Z가-힣]+)/n')
txt=SimplePos09(sentence)
txt_n <- str_match(txt, '([a-ZA-Z가-힣]+)/n')
txt_n <- str_match(txt, '([a-zA-Z가-힣]+)/N')
txt=SimplePos09(sen)
txt_n <- str_match(txt, '([a-zA-Z가-힣]+)/N')
txt_n
sen <- readLines("미국과중국.txt" , encoding="UTF-8")
extractNoun(sen)
sen <- readLines("미국과중국.txt")
extractNoun(sen)
sen <- readLines("미국과중국.txt")
extractNoun(sen)
sen <- extractNoun(sen)
sen
sen <- readLines("미국과중국.txt",encoding="UTF-8" )
extractNoun(sen)
sen
sen <- gsub("")
elitxt <- sen
for (i in 1:cnt) {
data_unlist <- gsub(elitxt[i],'', data_unlist)
}
cnt=length(elitxt)
elitxt[1]
for (i in 1:cnt) {
data_unlist <- gsub(elitxt[i],'', data_unlist)
}
txt=SimplePos22(sen)
txt_n <- str_match(txt, '([a-zA-Z가-힣]+)/N')
txt_n
txt_n <- str_match(txt, '([a-zA-Z가-힣]+)/N')
txt_n
##############
class(txt_n)
sen <- extractNoun(sen)
sen
### 쓸데없는 단어 삭제 및 빈도 확인
nouns <- gsub("\\d+","", nouns)
nouns <- gsub("\t","", nouns)
nouns <- gsub("[a-zA-Z]","", nouns)
nouns <- nouns[nchar(nouns)>=2]
nouns
wordFreq <- table(nouns)
wordFreq <- sort(wordFreq, decreasing=T)
len = length(wordFreq)
pal <- brewer.pal(8,"Dark2")
wc2<-wordcloud2(data=wdFreq1, size = 1.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wdFreq1 <- wordFreq[1:300]
wc2<-wordcloud2(data=wdFreq1, size = 1.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
#wdFreq50 <- wordFreq[1:30]
library(wordcloud2)
wc2<-wordcloud2(data=wdFreq1, size = 1.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
nouns <- sen
nouns <- unlist(nouns)
### 쓸데없는 단어 삭제 및 빈도 확인
nouns <- gsub("\\d+","", nouns)
nouns <- gsub("\t","", nouns)
nouns <- gsub("[a-zA-Z]","", nouns)
nouns <- gsub("한지","", nouns)
nouns <- nouns[nchar(nouns)>=2]
nouns
wordFreq <- table(nouns)
wordFreq <- sort(wordFreq, decreasing=T)
pal <- brewer.pal(8,"Dark2")
wc2<-wordcloud2(data=wdFreq1, size = 1.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
wordFreq <- table(nouns)
wordFreq <- sort(wordFreq, decreasing=T)
len = length(wordFreq)
wdFreq1 <- wordFreq[1:300]
wc2<-wordcloud2(data=wdFreq1, size = 1.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
wc2<-wordcloud2(data=wdFreq1, size = 2.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
wc2<-wordcloud2(data=wdFreq1, size =0.5, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
wc2<-wordcloud2(data=wdFreq1, size =1.0, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
nouns <- gsub("미국","", nouns)
nouns <- gsub("경제","", nouns)
nouns <- gsub("중국","", nouns)
nouns <- gsub("중국이","", nouns)
nouns <- nouns[nchar(nouns)>=2]
nouns
wordFreq <- table(nouns)
wordFreq <- sort(wordFreq, decreasing=T)
len = length(wordFreq)
wdFreq1 <- wordFreq[1:300]
wc2<-wordcloud2(data=wdFreq1, size =1.0, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
install.packages("tm")
library(tm)
docs=c("I am a boy", "You are a girl", "I am a student")
is(docs)
library(tm)
VectorSource(docs)
docs1 <- VectorSource(docs)
docs1
docs
myCorpus <- Corpus(VectorSource(docs))
myCorpus
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
findFreqTerms(tdm,2)
findFreqTerms(tdm,2.3)
docs=c("I am a boy", "You are a girl", "I am a student", "You ard a girl")
myCorpus <- Corpus(VectorSource(docs))
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
wordFreq <- sort(rowSums(m), decreasing=TRUE)
findFreqTerms(tdm,2.3)
docs=c("I am a boy", "You are a girl", "I am a student", "You are a girl")
docs1 <- VectorSource(docs)
myCorpus <- Corpus(VectorSource(docs))
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq
findFreqTerms(tdm,2.3)
wordFreq
findFreqTerms(tdm,2,3)
findFreqTerms(tdm,1)
findAssocs(tdm, "girl", 0.6)
findAssocs(tdm, "girl", 0.4)
findAssocs(tdm, "girl", 0.2)
findAssocs(tdm, "girl", 0.1)
m
findAssocs(tdm, "girl", 0.2)
findAssocs(tdm, "girl", 0.6)
m
findAssocs(tdm, "girl", 0.2)
findAssocs(tdm, "you", 0.2)
library(rvest)
all.reviews <-c()
url_all=c()
for (i in 1:7){
test <- paste0('https://edition.cnn.com/search/?size=10&q=Trump&from=i0&page=(i+1)',i)
url_all=c(url_all, test)
htxt <- read_html(url_all[i])
table <- html_nodes(htxt,'.review_info')
content <- html_node(table,'.desc_review')
reviews=html_text(content)
reviews <-gsub('\n','',reviews)
reviews <-gsub('\t','',reviews)
all.reviews <- c(all.reviews,reviews)
}
url_all
str(url_all)
str(table)
all.reviews
test <- paste0('https://edition.cnn.com/search/?size=10&q=Trump&from=10&page=2',i)
test
htxt <- read_html(test)
table <- html_nodes(htxt,'.review_info')
content <- html_node(table,'.desc_review')
content
htxt
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.review_info')
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
content <- html_node(htxt,'.cnn-search__result-body')
content
table <- xml_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
content <- xml_node(htxt,'.cnn-search__result-body')
content
test <- paste0('https://edition.cnn.com/search/?size=10&q=Trump&from=10&page=2',i)
test <- paste0('https://edition.cnn.com/search/?size=10&q=Trump&from=10&page=2')
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
content <- html_node(htxt,'.cnn-search__result-body')
content
test <-'https://edition.cnn.com/search/?size=10&q=Trump&from=10&page=2'
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
test <-'https://edition.cnn.com/2018/10/24/politics/pre-existing-conditions-midterms/index.html'
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt)
test <-'https://edition.cnn.com/search/?size=10&q=Trump&from=30&page=4'
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'cnn-search__results-pagi')
table
table <- html_nodes(htxt,'.cnn-search__results-pagi')
table
content <- html_node(htxt,'.cnn-search__result-body')
content
table <- html_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
cnn-search__results-pagi
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__results-pagi')
table
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__result-contents')
table
content <- html_node(htxt,'.cnn-search__result-body')
content
content <- html_node(htxt,'.cnn-search__result-contents')
content
test <- paste0('https://edition.cnn.com/search/?size=10&q=Trump&from=',i,'0&page=',i+1)
test
test <-'https://edition.cnn.com/2018/10/24/politics/donald-trump-saudi-arabia-wsj/index.html'
test
htxt <- read_html(test)
htxt
for (i in 1:7){
test <- paste0('https://movie.daum.net/moviedb/grade?movieId=112415&type=netizen&page=',i)
url_all=c(url_all, test)
htxt <- read_html(url_all[i])
table <- html_nodes(htxt,'.review_info')
content <- html_node(table,'.desc_review')
reviews=html_text(content)
reviews <-gsub('\n','',reviews)
reviews <-gsub('\t','',reviews)
all.reviews <- c(all.reviews,reviews)
}
url_all
str(url_all)
str(table)
all.reviews
library(rvest)
for (i in 1:33 ){
miss <- paste0('https://movie.daum.net/moviedb/grade?movieId=106812&type=netizen&page=',i)
all_url <- c(all_url,miss)
htxt1=read_html(all_url[i])
table1=html_nodes(htxt1,'.review_info')
content1=html_nodes(table1,'.desc_review')
reviews1 = html_text(content1)
reviews1 <-gsub('\n','',reviews1)
reviews1 <-gsub('\t','',reviews1)
all.reviews  <-c(all.reviews , reviews1)
}
all.reviews
sen
docs1 <- VectorSource(docs)
docs1 <- VectorSource(sen)
docs1
myCorpus <- Corpus(VectorSource(docs))
myCorpus
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus)
sen <- readLines("미국과중국.txt",encoding="UTF-8" )
sen
sen
docs1 <- VectorSource(sen)
docs1
myCorpus <- Corpus(VectorSource(docs))
docs <- VectorSource(sen)
myCorpus <- Corpus(VectorSource(docs))
myCorpus
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
trump <- readline("trump.txt", encoding='UTF-8')
setwd()
getwd()
trump <- readline("trump.txt", encoding='UTF-8')
trump <- readLines("trump.txt", encoding='UTF-8')
trump
trump <- VectorSource(trump)
docs1
docs <- trump
docs
myCorpus <- Corpus(VectorSource(docs))
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
inspect(myCorpus)
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
trump <- VectorSource(trump)
docs <- trump
myCorpus <- Corpus(VectorSource(docs))
myCorpus
trump <- readLines("trump.txt", encoding='UTF-8')
trump
docs <- trump
myCorpus <- Corpus(VectorSource(docs))
myCorpus
##불용어 처리 및 기타 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
inspect(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- as.matrix(tdm)
m
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq
findFreqTerms(tdm,1)
findAssocs(tdm, "embraces", 0.2)
wc2
wc2
wc2
wc2<-wordcloud2(data=wdFreq1, size =0.8, color = "random-light", backgroundColor = "grey",rotateRatio = 0.75)
wc2
test <- paste0('https://movie.daum.net/moviedb/grade?movieId=112415&type=netizen&page=',1)
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.review_info')
table
test <-'https://edition.cnn.com/2018/10/24/politics/donald-trump-saudi-arabia-wsj/index.html'
test
htxt <- read_html(test)
htxt
table <- html_nodes(htxt,'.cnn-search__result cnn-search__result--article')
table
table <- html_nodes(htxt,'.cnn-search__results-list')
table
table <- html_nodes(htxt,'cnn-search__results-list')
table
table <- html_nodes(htxt,'./cnn-search__results-list')
table <- html_nodes(htxt,'..cnn-search__results-list')
table <- html_nodes(htxt,'.cnn-search__results-list')
table
table <- html_nodes(htxt,'.cnn-search__result-thumbnail')
table
table <- html_nodes(htxt,'.cnn-search__result-contents')
table
table <- html_nodes(htxt,'."cnn-search__result-contents"')
table <- html_nodes(htxt,'.cnn-search__result-contents')
table
table <- html_nodes(htxt,'.cnn-search__result-body')
table
htxt
table <- html_nodes(htxt,'.fb_reset')
table
